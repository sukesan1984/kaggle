{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##記事が古すぎた！！！\n",
    "# http://curlyst.hatenablog.com/entry/2016/07/07/001722\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "import chainer\n",
    "from chainer import Variable, optimizers\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "# データインポート\n",
    "iris = datasets.load_iris()\n",
    "# irisのデータ見る\n",
    "pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris.target\n",
    "iris.target_names\n",
    "##\n",
    "\n",
    "#目的変数\n",
    "target_data = iris.target.astype(np.int32)\n",
    "\n",
    "# 説明変数\n",
    "# https://fisproject.jp/2016/06/data-standardization-using-python/\n",
    "# 標準化と正規化\n",
    "tmp_predictor_data = sp.stats.zscore(iris.data, axis=0)\n",
    "predictor_data =tmp_predictor_data.astype(np.float32)\n",
    "\n",
    "# 学習データとテストデータに分割\n",
    "test_size = 30\n",
    "data_num = len(target_data)\n",
    "\n",
    "# stratify: クラスを表す行列を設定\n",
    "# Stratified Sampling\n",
    "predictor_train, predictor_test, target_train, target_test = train_test_split(predictor_data, target_data, test_size=test_size, stratify=target_data)\n",
    "train_data_num = len(target_train)\n",
    "test_data_num = len(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "class MultiLayerPerceptron(chainer.Chain):\n",
    "    '''\n",
    "        モデルの定義\n",
    "    '''\n",
    "    def __init__(self, input_dim, n_units, output_dim, train=True, drop_out_ratio=0.3):\n",
    "        '''\n",
    "        コンストラクタ\n",
    "        '''\n",
    "        super(MultiLayerPerceptron, self).__init__(\n",
    "            l1=L.Linear(input_dim, n_units),\n",
    "            l2=L.Linear(n_units, n_units),\n",
    "            l3=L.Linear(n_units, output_dim)\n",
    "        )\n",
    "        self__train = train\n",
    "        self.__drop_out = True\n",
    "        self.drop_out_ratio = drop_out_ratio\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        順伝播計算\n",
    "        '''\n",
    "        drop_out = self.__train and self.__drop_out\n",
    "        h1 = F.dropout(F.relu(self.l1(x)), train=drop_out, ratio=self.drop_out_ratio)\n",
    "        h2 = F.dropout(F.relu(self.l2(h1)), train=drop_out, ratio=self.drop_out_ratio)\n",
    "        y = self.l3(h2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def __get_train(self):\n",
    "        return self.__train\n",
    "    \n",
    "    def __set_train(self, train):\n",
    "        self.__train = train\n",
    "        \n",
    "    train = property(__get_train, __set_train)\n",
    "    \n",
    "    # Dropoutを使用する場合: True\n",
    "    def __get_drop_out(self):\n",
    "        return self.__drop_out\n",
    "    \n",
    "    def __set_drop_out(self, drop_out):\n",
    "        '''\n",
    "        drop outフラグの設定\n",
    "        '''\n",
    "        self.__drop_out = drop_out\n",
    "        \n",
    "    drop_out = property(__get_drop_out, __set_drop_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchLearner:\n",
    "    '''\n",
    "    ミニバッチによる学習を行うクラス\n",
    "    '''\n",
    "    def __init__(self, optimizer, epoch_num, batch_size):\n",
    "        '''\n",
    "        コンストラクタ\n",
    "        '''\n",
    "        self.__optimizer = None\n",
    "        self.__epoch_num = None\n",
    "        self.__batch_size = None\n",
    "        self.__train_loss = None\n",
    "        self.__train_acc = None\n",
    "        self.__test_loss = None\n",
    "        self.__test_acc = None\n",
    "        # パラメータの初期化\n",
    "        self.set_params(optimizer, epoch_num, batch_size)\n",
    "        self.__init_loss_acc()\n",
    "        \n",
    "    def set_params(self, optimizer, epoch_num, batch_size):\n",
    "        self.__optimizer = optimizer\n",
    "        self.__epoch_num = epoch_num\n",
    "        self.__batch_size = batch_size\n",
    "        \n",
    "    def __init_loss_acc(self):\n",
    "        self.__train_loss = []\n",
    "        self.__train_acc = []\n",
    "        self.__test_loss = []\n",
    "        self.__test_acc = []\n",
    "        \n",
    "    def learn(self, model, predictor_train_data, target_train_data, drop_out=True):\n",
    "        '''\n",
    "        学習の実施\n",
    "        '''\n",
    "        self.__init_loss_acc()\n",
    "        # 学習データのインデックス(ランダム)\n",
    "        train_data_num = len(target_train_data)\n",
    "        perm = np.random.permutation(train_data_num)\n",
    "        \n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        \n",
    "        # 学習モード\n",
    "        model.predictor.train = True\n",
    "        model.predictor.drop_out = drop_out\n",
    "        \n",
    "        for idx in range(0, train_data_num, self.__batch_size):\n",
    "            predictor_batch = chainer.Variable(predictor_train_data[perm[idx:idx+self.__batch_size]])\n",
    "            target_batch = chainer.Variable(target_train_data[perm[idx:idx+self.__batch_size]])\n",
    "            \n",
    "            # 勾配を初期化\n",
    "            model.zerograds()\n",
    "            # 順伝播させて誤差と精度を算出\n",
    "            loss = model(predictor_batch, target_batch)\n",
    "            acc = model.accuracy\n",
    "            loss.backward()\n",
    "            self.__optimizer.update()\n",
    "            \n",
    "            self.__train_loss.append(loss.data)\n",
    "            self.__train_acc.append(acc.data)\n",
    "            sum_loss += float(loss.data) * len(target_batch)\n",
    "            sum_accuracy += float(acc.data) * len(target_batch)\n",
    "            \n",
    "        train_mean_loss = sum_loss / train_data_num\n",
    "        train_mean_acc = sum_accuracy / train_data_num\n",
    "        return train_mean_loss, train_mean_acc\n",
    "\n",
    "    def evaluate(self, model, predictor_test_data, target_test_data):\n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        \n",
    "        model.predictor.train = False\n",
    "        \n",
    "        predictor_batch = chainer.Variable(predictor_test_data)\n",
    "        target_batch = chainer.Variable(target_test_data)\n",
    "        \n",
    "        # 順伝播させて誤差と精度を算出\n",
    "        loss = model(predictor_batch, target_batch)\n",
    "        acc = model.accuracy\n",
    "        \n",
    "        test_data_num = len(target_test_data)\n",
    "        sum_loss = foat(loss.data) * test_data_num\n",
    "        sum_accuracy = float(acc.data) * test_data_num\n",
    "        \n",
    "        return float(loss.data), float(acc.data)\n",
    "        \n",
    "    train_mean_loss_list = []\n",
    "    train_mean_acc_list = []\n",
    "    test_mean_loss_list = []\n",
    "    test_mean_acc_list = []\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(batchsize, n_epoch, print_epoch, drop_out, drop_out_ratio, intermediate_layer_num):  \n",
    "    train_mean_loss_list = []\n",
    "    train_mean_acc_list = []\n",
    "    test_mean_loss_list = []\n",
    "    test_mean_acc_list = []\n",
    "\n",
    "    tmp_model = MultiLayerPerceptron(4, intermediate_layer_num, 3, drop_out_ratio=drop_out_ratio)\n",
    "    mlp_model = L.Classifier(tmp_model)\n",
    "\n",
    "    optimizer = optimizers.Adam()\n",
    "    optimizer.setup(mlp_model)\n",
    "\n",
    "    mb_learner = MiniBatchLearner(optimizer=optimizer, epoch_num=n_epoch, batch_size=batchsize)\n",
    "    for epoch in range(1, n_epoch+1):\n",
    "        if epoch % print_epoch == 0:\n",
    "            print('epoch', epoch)\n",
    "\n",
    "        # training\n",
    "        train_mean_loss, train_mean_acc = mb_learner.learn(mlp_model, predictor_train, target_train, drop_out)\n",
    "        if epoch % print_epoch == 0:\n",
    "            print('train mean loss={}, accuracy={}'.format(train_mean_loss, train_mean_acc))\n",
    "\n",
    "        # evaluation\n",
    "        test_mean_loss, test_mean_acc = mb_learner.evaluate(mlp_model, predictor_test, target_test)    \n",
    "        if epoch % print_epoch == 0:\n",
    "            print('test mean loss={}, accuracy={}'.format(test_mean_loss, test_mean_acc))\n",
    "\n",
    "        train_mean_loss_list.append(train_mean_loss)\n",
    "        train_mean_acc_list.append(train_mean_acc)\n",
    "        test_mean_loss_list.append(test_mean_loss)\n",
    "        test_mean_acc_list.append(test_mean_acc)\n",
    "    return train_mean_loss_list, train_mean_acc_list, test_mean_loss_list, test_mean_acc_list\n",
    "def draw_loss_and_acc(train_mean_loss_list, train_mean_acc_list, test_mean_loss_list, test_mean_acc_list, xlabel=\"epoch\", set_ylim=True):\n",
    "    fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # train\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    plt.plot(range(len(train_mean_loss_list)), train_mean_loss_list)\n",
    "    plt.plot(range(len(train_mean_acc_list)), train_mean_acc_list)\n",
    "    plt.legend([\"train_loss\",\"train_acc\"],loc=1)\n",
    "    plt.title(\"Loss / Accuracy of Iris class recognition.\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"loss/accracy\")\n",
    "    if set_ylim:\n",
    "        ax1.set_ylim([0, 1.2])\n",
    "    plt.grid()\n",
    "\n",
    "    # test\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    plt.plot(range(len(test_mean_loss_list)), test_mean_loss_list)\n",
    "    plt.plot(range(len(test_mean_acc_list)), test_mean_acc_list)\n",
    "    plt.legend([\"test_loss\",\"test_acc\"],loc=1)\n",
    "    plt.title(\"Loss / Accuracy of Iris class recognition.\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"loss/accracy\")\n",
    "    if set_ylim:\n",
    "        ax2.set_ylim([0, 1.2])\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "train argument is not supported anymore. Use chainer.using_config",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-61e5b15a4651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mintermediate_layer_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdrop_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_mean_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mean_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mean_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mean_acc_list\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_out_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintermediate_layer_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdraw_loss_and_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mean_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mean_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mean_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mean_acc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-97b8ffb1319f>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(batchsize, n_epoch, print_epoch, drop_out, drop_out_ratio, intermediate_layer_num)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_mean_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mean_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmb_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train mean loss={}, accuracy={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mean_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mean_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-2529bfc6f320>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, model, predictor_train_data, target_train_data, drop_out)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzerograds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# 順伝播させて誤差と精度を算出\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.0/lib/python3.6/site-packages/chainer/links/model/classifier.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlossfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-c5e5d6eb412e>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         '''\n\u001b[1;32m     23\u001b[0m         \u001b[0mdrop_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__drop_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_out_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_out_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.0/lib/python3.6/site-packages/chainer/functions/noise/dropout.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(x, ratio, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \"\"\"\n\u001b[1;32m    136\u001b[0m     argument.check_unexpected_kwargs(\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train argument is not supported anymore. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         'Use chainer.using_config')\n\u001b[1;32m    139\u001b[0m     \u001b[0margument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_kwargs_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.0/lib/python3.6/site-packages/chainer/utils/argument.py\u001b[0m in \u001b[0;36mcheck_unexpected_kwargs\u001b[0;34m(kwargs, **unexpected)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munexpected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: train argument is not supported anymore. Use chainer.using_config"
     ]
    }
   ],
   "source": [
    "batchsize = 10\n",
    "n_epoch = 2000\n",
    "print_epoch = n_epoch / 2\n",
    "drop_out_ratio = 0.3\n",
    "intermediate_layer_num=8\n",
    "drop_out = True\n",
    "train_mean_loss_list, train_mean_acc_list, test_mean_loss_list, test_mean_acc_list  = train_and_eval(batchsize, n_epoch, print_epoch, drop_out, drop_out_ratio, intermediate_layer_num)\n",
    "draw_loss_and_acc(train_mean_loss_list, train_mean_acc_list, test_mean_loss_list, test_mean_acc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
